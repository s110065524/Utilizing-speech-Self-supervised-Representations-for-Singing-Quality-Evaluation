{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill\n",
    "from decimal import Decimal\n",
    "data_path=  'D:/DAMP/Research/dilldata/CRNN_train_1.dill'\n",
    "\n",
    "data = dill.load(open(data_path, 'rb')) #list\n",
    "# print(len(data))\n",
    "\n",
    "# for item in data:\n",
    "#     wav = item['audio'][0]\n",
    "#     sr = item['audio'][1]\n",
    "#     wav = torch.tensor(wav)\n",
    "#     wav = wav[None, :].to(torch.float32)\n",
    "#     item['w2v_2'] = wav2vec_2(wav,sr)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "data_path=  'D:/DAMP/Research/dilldata/train_1.dill'\n",
    "\n",
    "data = dill.load(open(data_path, 'rb')) #list\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dill = open('D:/DAMP/Research/dilldata/w2v_CRNN_train_1.dill', 'wb') \n",
    "dill.dump(data, train_dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnse_embeddings_wav2vec_2 = []\n",
    "label = []\n",
    "label_str = []\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def features(feature_sequence,dim):\n",
    "# Create some random MFCC shaped features as a sequence of 10 values\n",
    "   # feature_sequence = np.random.random((10, dim))\n",
    "    # Get the standard deviation\n",
    "    stddev_features = np.std(feature_sequence, axis=0)\n",
    "    # Get the mean\n",
    "    mean_features = np.mean(feature_sequence, axis=0)\n",
    "    # Get the average difference of the features\n",
    "    average_difference_features = np.zeros((dim,))\n",
    "    for i in range(0, len(feature_sequence) - 2, 2):\n",
    "        average_difference_features += feature_sequence[i] - feature_sequence[i+1]\n",
    "    average_difference_features /= (len(feature_sequence) // 2)   \n",
    "    average_difference_features = np.array(average_difference_features)\n",
    "    # Concatenate the features to a single feature vector\n",
    "    concat_features_features = np.hstack((stddev_features, mean_features))\n",
    "    concat_features_features = np.hstack((concat_features_features, average_difference_features))\n",
    "    return concat_features_features\n",
    "\n",
    "\n",
    "# for item in data:\n",
    "#     rating = item['ratings'][0]\n",
    "#     if rating >= 0.6 or rating < -0.6:\n",
    "#         item = item['w2v_2'].numpy()\n",
    "#         item = np.squeeze(item, axis=0)\n",
    "#         item = features(item,dim=768)\n",
    "#         tnse_embeddings_wav2vec_2.append(item)\n",
    "        \n",
    "for item in data:\n",
    "    rating = item['ratings'][0]\n",
    "\n",
    "    item = item['w2v_2'].numpy()\n",
    "    item = np.squeeze(item, axis=0)\n",
    "    item = features(item,dim=768)\n",
    "    tnse_embeddings_wav2vec_2.append(item) \n",
    "\n",
    "count = [0,0,0,0,0]\n",
    "# for item in data:\n",
    "#     rating = item['ratings'][0]\n",
    "#     if rating >= 0.6: \n",
    "#         rating = 5\n",
    "#         rating_str = 'best singings'\n",
    "#         count[4]+=1\n",
    "#         label.append(rating)\n",
    "#         label_str.append(rating_str)\n",
    "#     elif rating < -0.6: \n",
    "#         rating = 1\n",
    "#         rating_str = 'worst singings'\n",
    "#         count[0]+=1\n",
    "#         label.append(rating)\n",
    "#         label_str.append(rating_str)\n",
    "for item in data:\n",
    "    rating = item['ratings'][0]\n",
    "    if rating >= 0.6: \n",
    "        rating = 5\n",
    "        rating_str = 'best singings'\n",
    "        count[4]+=1\n",
    "\n",
    "    elif rating >= 0.2: \n",
    "        rating = 4\n",
    "        count[3]+=1\n",
    "    elif rating >= -0.2: \n",
    "        rating = 3\n",
    "        count[2]+=1\n",
    "    elif rating >= -0.6: \n",
    "        rating = 2\n",
    "        count[1]+=1\n",
    "    elif rating < -0.6: \n",
    "        rating = 1\n",
    "        rating_str = 'worst singings'\n",
    "        count[0]+=1\n",
    "    label.append(rating)\n",
    "\n",
    "count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label))\n",
    "len(tnse_embeddings_wav2vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas\n",
    "def get_scaled_tsne_embeddings(features, perplexity, iteration):\n",
    "    embedding = TSNE(n_components=2,\n",
    "                     perplexity=perplexity,\n",
    "                     n_iter=iteration).fit_transform(features)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(embedding)\n",
    "    return scaler.transform(embedding) \n",
    "\n",
    "all_json = dict()\n",
    "tnse_embeddings_mfccs = []\n",
    "perplexities = [2, 5, 30, 50, 100,150,250,500]\n",
    "iterations = [250, 500, 1000, 2000, 5000,8000,10000,15000]\n",
    "\n",
    "for i,perplexity in enumerate(perplexities):\n",
    "    for j, iteration in enumerate(iterations):\n",
    "        tsne_mfccs = get_scaled_tsne_embeddings(tnse_embeddings_wav2vec_2,\n",
    "                                                perplexity,\n",
    "                                                iteration)\n",
    "        tnse_embeddings_mfccs.append(tsne_mfccs)\n",
    "\n",
    "        \n",
    "        mfcc_key = 'tsnemfcc{}{}'.format(i, j) \n",
    "        wavenet_key = 'tsnewavenet{}{}'.format(i, j) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(perplexities)):\n",
    "    for j in range(len(iterations )):\n",
    "        current_plot = i * 8 + j\n",
    "        \n",
    "\n",
    "        Feature_1 = tnse_embeddings_mfccs[current_plot].T[0]\n",
    "        Feature_2 = tnse_embeddings_mfccs[current_plot].T[1]\n",
    "\n",
    "        dataframe = {\"Feature_1\":Feature_1,\n",
    "                \"Feature_2\":Feature_2,\n",
    "                'label':label\n",
    "                }\n",
    "        df = pandas.DataFrame(dataframe)\n",
    "        # colors = {'best singings':'tab:red', 'worst singings':'tab:green'}\n",
    "        # fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        # ax.scatter(df['Feature_1'], df['Feature_2'], c=df['label'].map(colors))\n",
    "        # plt.show()\n",
    "\n",
    "        df.plot(x=\"Feature_1\", y=\"Feature_2\",xlabel='',c='label', ylabel='',kind='scatter', colormap='viridis')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = manifold.TSNE(n_components=2, init='random', random_state=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import IPython\n",
    "import requests\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def wav2vec_2(wav,sr):\n",
    "    torch.random.manual_seed(123)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "    model = bundle.get_model().to(device)\n",
    "\n",
    "    #SPEECH_FILE   = \"D:/DAMP/SingEval_DAMPcurated/audio_snippets_1/_cups_pitch_perfect/68307032_39376090.wav\"\n",
    "    #SPEECH_FILE   = \"D:/DAMP/SingEval_DAMPcurated/audio_snippets_1/_let_it_go/70366197_41259492.wav\"\n",
    "    bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "    # print(\"Sample Rate:\", bundle.sample_rate)\n",
    "    # print(model.__class__)\n",
    "    waveform, sample_rate =wav ,sr\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    with torch.inference_mode():\n",
    "        features, _ = model(waveform)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_FILE   = \"D:/DAMP/SingEval_DAMPcurated/audio_snippets_1/_cups_pitch_perfect/68307032_39376090.wav\"\n",
    "\n",
    "\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "print(type(waveform))\n",
    "print(waveform.shape)\n",
    "print(type(sample_rate))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2674b10023feeb8169b3ca32823a0f2a9d1f15b90bed60b7cd2828d723772438"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ailab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
